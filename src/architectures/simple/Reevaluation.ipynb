{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: Tobias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some mistakes in the NanWrapper and metric calculation, we had to reevaluate all our models. We use this notebook to load previously trained models and reevalute their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import tensorflow as tf\n",
    "\n",
    "basepath = Path(os.getcwd())\n",
    "# make sure your working directory is the repository root.\n",
    "if basepath.name != \"idp-radio-1\":\n",
    "    os.chdir(basepath.parent.parent.parent)\n",
    "load_dotenv(find_dotenv())\n",
    "basepath = Path(os.getcwd())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which GPU(s) to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Or 2, 3, etc. other than 0\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto(device_count={'GPU': 1}, allow_soft_placement=True, log_device_placement=True)\n",
    "config = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.2\n",
    "tf.compat.v1.Session(config=config)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import traceback\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "from tensorflow.keras.applications import InceptionV3, Xception, DenseNet121, InceptionResNetV2, ResNet152V2, NASNetLarge\n",
    "from src.architectures.simple.simple_base import SimpleBaseArchitecture\n",
    "from src.architectures.simple.load_model import *\n",
    "from src.architectures.benchmarks.benchmark import Benchmark, Experiment\n",
    "from src.architectures.benchmarks.benchmark_definitions import generate_benchmarks,simple_architecture_experiment, Chexpert_Benchmark, CHEXPERT_COLUMNS, METRICS, SINGLE_CLASS_METRICS\n",
    "from src.metrics.metrics import F2Score, SingleClassMetric\n",
    "from src.metrics.losses import WeightedBinaryCrossentropy, compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = get_all_experiment_logs()\n",
    "experiments = [exp for exp in experiments if \"Failed\" not in exp[\"name\"]]\n",
    "#experiments = [exp for exp in experiments if \"N12\" in exp[\"name\"]]\n",
    "experiments = [exp for exp in experiments if \"num_samples_test\" in exp[\"benchmark\"].keys() ]\n",
    "experiments = [exp for exp in experiments if exp[\"benchmark\"][\"num_samples_test\"] == 234 ]\n",
    "len(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict = experiments[-2].copy()\n",
    "exp_dict[\"name\"], exp_dict[\"test\"][\"auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reevaluate model {} - {} \".format(exp_dict[\"name\"], exp_dict[\"id\"]), \"\\n\")\n",
    "exp = rebuild_experiment(exp_dict)\n",
    "exp_dict[\"test_again\"] = reevaluate(exp, new_metrics=True)\n",
    "print(exp_dict[\"test\"][\"auc\"] - exp_dict[\"test_again\"][\"auc\"])\n",
    "print(\"sum difference\", sum(difference_test_results(exp_dict[\"test\"], exp_dict[\"test_again\"]).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_experiments = []\n",
    "for index, exp_dict in enumerate(experiments[:]):\n",
    "    for epoch_id in range(exp_dict[\"benchmark\"][\"epochs\"]):\n",
    "        new_exp_dict = exp_dict.copy()\n",
    "        new_exp_dict[\"epoch_model\"] = epoch_id + 1\n",
    "        try:\n",
    "            print(\"\\nReevaluate model {} - {} \".format(index, new_exp_dict[\"id\"]))\n",
    "            print(\"For Epoch {} \".format(new_exp_dict[\"epoch_model\"]))\n",
    "            \n",
    "            exp = rebuild_experiment(exp_dict, epoch=new_exp_dict[\"epoch_model\"])\n",
    "            new_exp_dict[\"test_again\"] = reevaluate(exp, new_metrics=True)\n",
    "            new_exp_dict[\"val_again\"] = reevaluate_validation(exp, new_metrics=True)\n",
    "            new_exp_dict[\"val\"] = {name.replace(\"val_\", \"\"):res[-1] for name, res in new_exp_dict[\"history\"].items() if name.startswith(\"val_\")}\n",
    "\n",
    "            new_experiments.append(new_exp_dict)\n",
    "\n",
    "            print(\"test_again auc is \", new_exp_dict[\"test_again\"][\"auc\"])\n",
    "            print(\"test auc is \", new_exp_dict[\"test\"][\"auc\"])\n",
    "            print(\"val_again auc is \", new_exp_dict[\"val_again\"][\"auc\"])\n",
    "            print(\"val auc is \", new_exp_dict[\"val\"][\"auc\"])\n",
    "\n",
    "            del exp\n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\n",
    "    \"name\":new_experiment[\"name\"], \n",
    "    \"id\":new_experiment[\"id\"],\n",
    "    \"epoch\": new_experiment[\"epoch_model\"],\n",
    "    \"test auc\":new_experiment[\"test\"][\"auc\"],\n",
    "    \"test again auc\":new_experiment[\"test_again\"][\"auc\"],\n",
    "    \"val auc\":new_experiment[\"val\"][\"auc\"] if \"auc\" in new_experiment[\"val\"].keys() else None, \n",
    "    \"val again auc\": new_experiment[\"val_again\"][\"auc\"]\n",
    " }\n",
    "    for new_experiment in new_experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[new_experiment[\"test_again\"][\"auc\"] for new_experiment in new_experiments], [new_experiment[\"test\"][\"auc\"] for new_experiment in new_experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast from np.float32 to float:\n",
    "for exp in new_experiments:\n",
    "    exp[\"test_again\"] = {k:float(v) for k, v in exp[\"test_again\"].items()}\n",
    "    exp[\"val_again\"] = {k:float(v) for k, v in exp[\"val_again\"].items()}\n",
    "\n",
    "output_file = Path(\"/srv/idp-radio-1/logs/epoch_reevaluation_experiments.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    data = {\"experiments\":new_experiments}\n",
    "    json_data = json.dumps(data, indent=4)\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in new_experiments:\n",
    "    classes = [classname.lower().replace(\" \", \"_\") for classname in exp[\"benchmark\"][\"label_columns\"]]\n",
    "    if len(classes) == 12:\n",
    "        #print([exp[\"test_again\"][\"auc_\"+classname]for classname in classes])\n",
    "        exp[\"test_again\"][\"auc\"] = sum([exp[\"test_again\"][\"auc_\"+classname]for classname in classes])/(len(classes)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [(new_experiment[\"id\"], new_experiment[\"epoch_model\"], new_experiment[\"test_again\"][\"auc\"]) for new_experiment in new_experiments if \"N12\" in new_experiment[\"name\"]]\n",
    "res.sort(key=lambda tup: tup[2])\n",
    "res.reverse()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.benchmark.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
