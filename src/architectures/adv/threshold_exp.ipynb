{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.0-dev20200815'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import tensorflow\n",
    "\n",
    "basepath = Path(os.getcwd())\n",
    "# make sure your working directory is the repository root.\n",
    "if basepath.name != \"idp-radio-1\":\n",
    "    os.chdir(basepath.parent.parent.parent)\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "os.getcwd()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Or 2, 3, etc. other than 0\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto(device_count={'GPU': 2}, allow_soft_placement=True, log_device_placement=True)\n",
    "config = tensorflow.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.8\n",
    "tensorflow.compat.v1.Session(config=config)\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras import models\n",
    "import json\n",
    "from src.architectures.benchmarks.benchmark import Benchmark, Experiment\n",
    "from pathlib import Path\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from src.metrics.losses import WeightedBinaryCrossentropy, compute_class_weight\n",
    "\n",
    "from tensorflow.keras.backend import manual_variable_initialization \n",
    "manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY WHICH EXPERIMENT TO LOAD BY INDEX IN EXPERIMENT-LOG.JSON\n",
    "ind = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chexpert_BCE_E5_B52_C0_N5_D256_DS9505_LR4\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "with open(\"logs/unvalidated-experiment-log.json\") as f:\n",
    "  experiments = json.load(f)\n",
    "experiment = experiments['experiments'][ind]\n",
    "experiment_benchmark = experiments['experiments'][ind]['benchmark']\n",
    "\n",
    "experiment_benchmark['name'] = experiment_benchmark['benchmark_name']\n",
    "experiment_benchmark['dataset_folder'] = Path(experiment_benchmark['dataset_folder'])\n",
    "\n",
    "experiment_benchmark.pop('benchmark_name', None)\n",
    "experiment_benchmark.pop('num_samples_train', None)\n",
    "experiment_benchmark.pop('num_samples_validation', None)\n",
    "experiment_benchmark.pop('num_samples_test', None)\n",
    "\n",
    "if experiment_benchmark['optimizer'] == 'Adam':\n",
    "    lr = experiment_benchmark['learning_rate']\n",
    "    experiment_benchmark['optimizer'] = Adam(learning_rate=lr)\n",
    "    experiment_benchmark.pop('learning_rate', None)\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "posw = tensorflow.constant(experiment_benchmark['positive_weights'], dtype=tensorflow.float32)\n",
    "negw = tensorflow.constant(experiment_benchmark['negative_weights'], dtype=tensorflow.float32)\n",
    "\n",
    "experiment_benchmark.pop('positive_weights', None)\n",
    "experiment_benchmark.pop('negative_weights', None)\n",
    "\n",
    "experiment_benchmark['test_labels'] = \"test.csv\"\n",
    "\n",
    "benchmark = Benchmark(**experiment_benchmark)\n",
    "testgen = benchmark.testgen\n",
    "traingen = benchmark.traingen\n",
    "valgen = benchmark.valgen\n",
    "\n",
    "print(experiment_benchmark['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from src.metrics.metrics import SingleClassMetric, NaNWrapper\n",
    "\n",
    "path = os.path.join(\"models/\", experiment['name'], experiment['filename'])\n",
    "reconstructed_model = models.load_model(path, compile=False)\n",
    "\n",
    "if experiment_benchmark['loss'] == \"weighted_binary_crossentropy\":\n",
    "    loss = WeightedBinaryCrossentropy(posw, negw)\n",
    "elif experiment_benchmark['loss'] == \"binary_crossentropy\":\n",
    "    loss = BinaryCrossentropy()\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "reconstructed_model.compile(optimizer=Adam(), loss=loss, metrics=[Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "# testgen.on_epoch_end()\n",
    "print(len(testgen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_class_weight(traingen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 11s 47ms/step\n",
      "current threshold:  0.3\n",
      "TF Precision:  tf.Tensor(0.33027524, shape=(), dtype=float32)\n",
      "TF Recall:  tf.Tensor(0.12286689, shape=(), dtype=float32)\n",
      "current threshold:  0.4\n",
      "TF Precision:  tf.Tensor(0.35555556, shape=(), dtype=float32)\n",
      "TF Recall:  tf.Tensor(0.10921501, shape=(), dtype=float32)\n",
      "current threshold:  0.5\n",
      "TF Precision:  tf.Tensor(0.3561644, shape=(), dtype=float32)\n",
      "TF Recall:  tf.Tensor(0.088737205, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "groundtruth_label = testgen.get_labels_nonan()\n",
    "predictions = reconstructed_model.predict(testgen, steps=len(testgen), verbose=1)\n",
    "\n",
    "for t in thresholds:\n",
    "    print('current threshold: ', t)\n",
    "    predictions_bool = (predictions >= t)\n",
    "    y_pred = np.array(predictions_bool, dtype=int)\n",
    "\n",
    "    # report = classification_report(groundtruth_label, y_pred, target_names=list(CHEXPERT_BENCHMARKS['WBCE_E5_B32_C0'].label_columns))\n",
    "    # print('sklearn report: ', report)\n",
    "    \n",
    "    pre = Precision(thresholds=t)\n",
    "    pre.update_state(groundtruth_label, predictions_bool)\n",
    "    print('TF Precision: ', pre.result())\n",
    "    pre.reset_states()\n",
    "    \n",
    "    rec = Recall(thresholds=t)\n",
    "    rec.update_state(groundtruth_label, predictions_bool)\n",
    "    print('TF Recall: ', rec.result())\n",
    "    rec.reset_states()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions\n",
    "groundtruth_label = testgen.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(groundtruth_label, y_pred, target_names=benchmark.label_columns)\n",
    "print('sklearn report: ', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = reconstructed_model.evaluate(\n",
    "            x=testgen, steps=len(testgen), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in predictions:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('models/DenseNet121_Chexpert_BCE_E5_B52_C0_N5_D256_DS9505_LR4/predictions_probs.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn report:                    precision    recall  f1-score   support\n",
      "\n",
      "    Cardiomegaly       0.25      0.01      0.03        68\n",
      "           Edema       0.12      0.09      0.10        45\n",
      "   Consolidation       0.00      0.00      0.00        33\n",
      "     Atelectasis       0.67      0.03      0.05        80\n",
      "Pleural Effusion       0.35      0.27      0.30        67\n",
      "\n",
      "       micro avg       0.26      0.09      0.13       293\n",
      "       macro avg       0.28      0.08      0.10       293\n",
      "    weighted avg       0.34      0.09      0.10       293\n",
      "     samples avg       0.09      0.05      0.06       293\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions_bool_mod = (my_data >= 0.5)\n",
    "y_pred_mod = np.array(predictions_bool_mod, dtype=int)\n",
    "\n",
    "report = classification_report(groundtruth_label, y_pred_mod, target_names=benchmark.label_columns)\n",
    "print('sklearn report: ', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(predictions_bool))\n",
    "np.sum(predictions_bool_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think something doesnt work with the masked loss function, I get really low values here. Fix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
