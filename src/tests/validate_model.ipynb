{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "basepath = Path(os.getcwd())\n",
    "if basepath.name != \"idp-radio-1\":\n",
    "    os.chdir(basepath.parent.parent)\n",
    "    print(os.getcwd())\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from src.preprocessing.split.train_test_split import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(os.environ['EXP_DATA'])\n",
    "history = data['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Benchmark Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in data[\"description\"].split(\".\")[:-1]:\n",
    "    print(s + \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and format metrics to be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any metrics that were renamed, add this new name here as (\"default_name\":\"new_name\")\n",
    "metric_custom_names={\"auc\":\"AUC_ROC\"}\n",
    "\n",
    "metric_names = [re.sub(\"([a-z0-9])([A-Z])\",\"\\g<1> \\g<2>\",name) for name in data[\"benchmark\"][\"metrics\"]]\n",
    "metric_keys = [re.sub(\"([a-z0-9])([A-Z])\",\"\\g<1>_\\g<2>\",name).lower() for name in data[\"benchmark\"][\"metrics\"]]\n",
    "\n",
    "for default_name, custom_name in metric_custom_names.items():\n",
    "    if not default_name in history.keys() and default_name in metric_keys:\n",
    "        #replace default name with custom name\n",
    "        metric_keys[metric_keys.index(default_name)]=custom_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_or_plot_metric(metric_key, metric_name, figure_name):\n",
    "    if len(history[metric_key]) == 1:\n",
    "        print(\"Data for {m_name} only available for a single epoch. \\nSkipping plot and printing data...\".format(m_name=metric_name))\n",
    "        print('Train {}: '.format(metric_name), history[metric_key])\n",
    "        print('Validation {}: '.format(metric_name), history['val_'+metric_key])\n",
    "        print()        \n",
    "    else:\n",
    "        plot_epoch_metric(metric_key, metric_name, figure_name)\n",
    "        \n",
    "def plot_epoch_metric(metric_key, metric_name, figure_name):\n",
    "    figure(num=None, figsize=(10, 6))\n",
    "    plt.plot(history[metric_key])\n",
    "    if 'val_'+metric_key in history.keys():\n",
    "        plt.plot(history['val_'+metric_key])\n",
    "    plt.title(figure_name)\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    if 'val_'+metric_key in history.keys():\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training & validation loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_or_plot_metric(\"loss\", \"Loss\", \"Model loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lr\" in history.keys():\n",
    "    plot_epoch_metric(\"lr\", \"Learning Rate\", \"Learning Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, scores in history.items():\n",
    "    if \"val_\" in name:\n",
    "        print('Valid {}: '.format(name.replace(\"val_\", \"\")), scores[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if 'test' in data.keys() and data['test']:\n",
    "    for score_name, score in data[\"test\"].items():\n",
    "        print('Test {}: '.format(score_name), score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'classification_report' in data.keys() and data['classification_report']:\n",
    "    print(data['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "if \"benchmark\" in data.keys():\n",
    "    pp.pprint(data[\"benchmark\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'benchmark' in data.keys() and 'split_seed' in data['benchmark']:\n",
    "    benchmark = data['benchmark']\n",
    "\n",
    "    dataset_path = Path(benchmark['dataset_folder'])\n",
    "    train_labels = benchmark['train_labels'] if 'train_labels' in benchmark.keys() else 'train.csv'\n",
    "    test_labels = benchmark['test_labels'] if 'test_labels' in benchmark.keys() else None\n",
    "    split_test_size =  benchmark['split_test_size'] if 'split_test_size' in benchmark.keys() else 0.1\n",
    "    split_valid_size =  benchmark['split_valid_size'] if 'split_valid_size' in benchmark.keys() else 0.1\n",
    "    split_group = benchmark['split_group'] if 'split_group' in benchmark.keys() else 'patient_id'\n",
    "    split_seed = benchmark['split_seed']\n",
    "\n",
    "    if test_labels is None:\n",
    "        # read all labels from one file and split into train/test/valid\n",
    "        all_labels = pd.read_csv(dataset_path / train_labels)\n",
    "        train_labels, test_labels = train_test_split(\n",
    "            all_labels, test_size=split_test_size, group=split_group, seed=split_seed)\n",
    "        train_labels, validation_labels = train_test_split(\n",
    "            train_labels, test_size=split_valid_size, group=split_group, seed=split_seed)\n",
    "    else:\n",
    "        # read train and valid labels from one file and test from another.\n",
    "        train_labels = pd.read_csv(dataset_path / train_labels)\n",
    "        train_labels, validation_labels = train_test_split(\n",
    "            train_labels, test_size=split_valid_size, group=split_group, seed=split_seed)\n",
    "        test_labels = pd.read_csv(dataset_path / test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.u_encoding import uencode\n",
    "\n",
    "def get_distribution(labels):\n",
    "    if 'nan_replacement' in benchmark.keys():\n",
    "        labels = labels.fillna(benchmark['nan_replacement'])\n",
    "    data = labels.to_numpy()\n",
    "    data = uencode(benchmark['u_enc'], data, unc_value=benchmark['unc_value'])\n",
    "    data = pd.DataFrame(data, columns=labels.columns)\n",
    "\n",
    "    labels = data[benchmark['label_columns']]\n",
    "\n",
    "    d = {'Pathology': [], 'Positive': [], 'Positive %': [], 'Negative': [], 'Negative %': [],}\n",
    "    for label in labels.columns:\n",
    "        values = labels.groupby(label)\n",
    "        d['Pathology'].append(label)\n",
    "\n",
    "        positive = values.size()[1.0] if 1.0 in values.size() else 0\n",
    "        positive_percent = positive / labels.shape[0] * 100\n",
    "        d['Positive'].append(positive)\n",
    "        d['Positive %'].append(round(positive_percent))\n",
    "\n",
    "        negative = values.size()[-0.0] if -0.0 in values.size() else 0\n",
    "        negative_percent = negative / labels.shape[0] * 100\n",
    "        d['Negative'].append(negative)\n",
    "        d['Negative %'].append(round(negative_percent))\n",
    "    \n",
    "    df = pd.DataFrame(d)\n",
    "    df = df.set_index('Pathology')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'benchmark' in data.keys() and 'split_seed' in data['benchmark']:\n",
    "    train = get_distribution(train_labels)\n",
    "    val = get_distribution(validation_labels)\n",
    "    test = get_distribution(test_labels)\n",
    "    \n",
    "    positives = train[['Positive %']].merge(val[['Positive %']], left_index=True, right_index=True).merge(test[['Positive %']], left_index=True,  right_index=True).rename(columns={\"Positive %_x\": \"Positives Train\", \"Positive %_y\": \"Positives Validation\", \"Positive %\": \"Positives Test\", })\n",
    "    positives.copy().plot(kind='bar', figsize=(10,7), title=\"Positive Labels Distribution\")\n",
    "    \n",
    "    negatives = train[['Negative %']].merge(val[['Negative %']], left_index=True, right_index=True).merge(test[['Negative %']], left_index=True,  right_index=True).rename(columns={\"Negative %_x\": \"Negative Train\", \"Negative %_y\": \"Negative Validation\", \"Negative %\": \"Negative Test\", })\n",
    "    negatives.copy().plot(kind='bar', figsize=(10,7), title=\"Negative Labels Distribution\")\n",
    "\n",
    "    train[['Positive %', 'Negative %']].copy().plot(kind='bar', figsize=(10,7), title=\"Training set\")\n",
    "    val[['Positive %', 'Negative %']].copy().plot(kind='bar', figsize=(10,7), title=\"Validation set\")\n",
    "    test[['Positive %', 'Negative %']].copy().plot(kind='bar', figsize=(10,7), title=\"Test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training & validation accuracy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, metric_key in enumerate(metric_keys):\n",
    "    print_or_plot_metric(metric_key, metric_names[i], \"Model \"+metric_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
