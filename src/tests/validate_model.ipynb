{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(os.environ['EXP_DATA'])\n",
    "history = data['history']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Benchmark Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in data[\"description\"].split(\".\")[:-1]:\n",
    "    print(s + \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and format metrics to be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any metrics that were renamed, add this new name here as (\"default_name\":\"new_name\")\n",
    "metric_custom_names={\"auc\":\"AUC_ROC\"}\n",
    "\n",
    "metric_names = [re.sub(\"([a-z0-9])([A-Z])\",\"\\g<1> \\g<2>\",name) for name in data[\"benchmark\"][\"metrics\"]]\n",
    "metric_keys = [re.sub(\"([a-z0-9])([A-Z])\",\"\\g<1>_\\g<2>\",name).lower() for name in data[\"benchmark\"][\"metrics\"]]\n",
    "\n",
    "for default_name, custom_name in metric_custom_names.items():\n",
    "    if not default_name in history.keys() and default_name in metric_keys:\n",
    "        #replace default name with custom name\n",
    "        metric_keys[metric_keys.index(default_name)]=custom_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training & validation accuracy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_or_plot_metric(metric_key, metric_name, figure_name):\n",
    "    if len(history[metric_key]) == 1:\n",
    "        print(\"Data for {m_name} only available for a single epoch. \\nSkipping plot and printing data...\".format(m_name=metric_name))\n",
    "        print('Train {}: '.format(metric_name), history[metric_key])\n",
    "        print('Validation {}: '.format(metric_name), history['val_'+metric_key])\n",
    "        print()        \n",
    "    else:\n",
    "        plot_epoch_metric(metric_key, metric_name, figure_name)\n",
    "        \n",
    "def plot_epoch_metric(metric_key, metric_name, figure_name):\n",
    "    figure(num=None, figsize=(10, 6))\n",
    "    plt.plot(history[metric_key])\n",
    "    if 'val_'+metric_key in history.keys():\n",
    "        plt.plot(history['val_'+metric_key])\n",
    "    plt.title(figure_name)\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    if 'val_'+metric_key in history.keys():\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "for i, metric_key in enumerate(metric_keys):\n",
    "    print_or_plot_metric(metric_key, metric_names[i], \"Model \"+metric_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training & validation loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_or_plot_metric(\"loss\", \"Loss\", \"Model loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lr\" in history.keys():\n",
    "    plot_epoch_metric(\"lr\", \"Learning Rate\", \"Learning Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'classification_report' in data.keys() and data['classification_report']:\n",
    "    print(data['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if 'test' in data.keys() and data['test']:\n",
    "    for score_name, score in data[\"test\"].items():\n",
    "        print('Test {}: '.format(score_name), score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "if \"benchmark\" in data.keys():\n",
    "    pp.pprint(data[\"benchmark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
